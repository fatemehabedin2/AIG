{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fatemehabedin2/AIG100/blob/main/Lab_1_Basics_of_TensorFlow_and_PyTorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HgY6vfNsztJe"
      },
      "source": [
        "# Week 3 Lab - Basics of TensorFlow and PyTorch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GNPUVVTeztJh"
      },
      "source": [
        "## Verifying GPU Availability"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9fV69scZztJi"
      },
      "source": [
        "### TensorFlow GPU Check"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PkYZzdClztJj",
        "outputId": "af86f828-3a92-44ef-e528-1aca18f9f287"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TensorFlow version: 2.18.0\n",
            "GPU is available for TensorFlow!\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "print(\"TensorFlow version:\", tf.__version__)\n",
        "gpu_devices = tf.config.list_physical_devices('GPU')\n",
        "if gpu_devices:\n",
        "    print(\"GPU is available for TensorFlow!\")\n",
        "else:\n",
        "    print(\"No GPU found for TensorFlow.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_LctknOGztJk"
      },
      "source": [
        "### PyTorch GPU Check"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V8LMdM7pztJk",
        "outputId": "43247a85-1cd2-46d9-84cf-aa13fdda4ddb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PyTorch version: 2.6.0+cu124\n",
            "GPU is available for PyTorch!\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "print(\"PyTorch version:\", torch.__version__)\n",
        "if torch.cuda.is_available():\n",
        "    print(\"GPU is available for PyTorch!\")\n",
        "else:\n",
        "    print(\"No GPU found for PyTorch.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CQmevqa1ztJl"
      },
      "source": [
        "## Creating and Training a Simple Neural Network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UgWIP4kvztJl"
      },
      "source": [
        "### TensorFlow Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OCkiCOjYztJm",
        "outputId": "eda4143e-be55-4641-a51d-6d7ea3ed8aaa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "\u001b[1m11490434/11490434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/reshaping/flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 4ms/step - accuracy: 0.8799 - loss: 0.4262 - val_accuracy: 0.9543 - val_loss: 0.1476\n",
            "Epoch 2/5\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.9635 - loss: 0.1209 - val_accuracy: 0.9707 - val_loss: 0.1018\n",
            "Epoch 3/5\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.9778 - loss: 0.0748 - val_accuracy: 0.9719 - val_loss: 0.0920\n",
            "Epoch 4/5\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 3ms/step - accuracy: 0.9828 - loss: 0.0547 - val_accuracy: 0.9755 - val_loss: 0.0778\n",
            "Epoch 5/5\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.9867 - loss: 0.0432 - val_accuracy: 0.9774 - val_loss: 0.0761\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9751 - loss: 0.0894\n",
            "Test accuracy: 0.9774\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Flatten\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# Load and preprocess the MNIST dataset\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "x_train = x_train / 255.0     #levels of gray in each pixel  ... normalize the pixel value (0-1). 255 is the maximum pixel value in 8-bit grayscale images.\n",
        "x_test = x_test / 255.0\n",
        "y_train = to_categorical(y_train)     # Converts the labels (which are integers 0 to 9) into one-hot encoded vectors.\n",
        "y_test = to_categorical(y_test)\n",
        "\n",
        "# Define the model\n",
        "model = Sequential([            # initializes a sequential model, which means layers are stacked one after the other in a linear fashion.\n",
        "    Flatten(input_shape=(28, 28)),   # change the 2D to 1D vector. Converts each 28×28 2D image into a 1D vector of length 784.This is necessary because dense (fully connected) layers expect a 1D input.\n",
        "    Dense(128, activation='relu'),   # Dense, A fully connected layer with 128 neurons. relu (Rectified Linear Unit) is used as the activation function:It introduces non-linearity. Formula: relu(x) = max(0, x). Learns 128 different features from the input.\n",
        "    Dense(10, activation='softmax')   # Output layer with 10 neurons (one for each digit class: 0–9). Softmax converts raw scores into probabilities that sum to 1. The neuron with the highest value represents the model’s predicted digit.\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(          # configures the model for training.\n",
        "    optimizer='adam',           # how the model updates its weights. Adam = Adaptive Moment Estimation. A very popular and efficient optimizer that combines the benefits of:Momentum (smooths updates using past gradients), RMSProp (scales learning rate based on past squared gradients). Usually works well out-of-the-box for many deep learning tasks.\n",
        "    loss='categorical_crossentropy',  # is used for multi-class classification with one-hot encoded labels. It measures how far the predicted probabilities are from the true labels.\n",
        "    metrics=['accuracy'])   # tracks the percentage of correct predictions\n",
        "\n",
        "# Train the model\n",
        "with tf.device('/GPU:0'):   # a TensorFlow device context manager that tells TensorFlow to run the operations inside the block on GPU #0 (the first available GPU).\n",
        "    model.fit(x_train, y_train, epochs=5, batch_size=32, validation_data=(x_test, y_test))   # recommended to use validation_data=(x_val, y_val) to prevent data leakage\n",
        "\n",
        "# Evaluate the model\n",
        "loss, accuracy = model.evaluate(x_test, y_test)   # evaluates the model on the entire test set once, after all training epochs are complete. It feeds all test images through the final trained version of your model (after epoch 5). It then calculates loss, accuracy\n",
        "print(f'Test accuracy: {accuracy:.4f}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PbvYS9x1ztJm"
      },
      "source": [
        "### PyTorch Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "REWBf0hhztJn",
        "outputId": "a3c000c0-253d-4dc4-ead4-3ed12b1e8836"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9.91M/9.91M [00:00<00:00, 18.1MB/s]\n",
            "100%|██████████| 28.9k/28.9k [00:00<00:00, 489kB/s]\n",
            "100%|██████████| 1.65M/1.65M [00:00<00:00, 4.59MB/s]\n",
            "100%|██████████| 4.54k/4.54k [00:00<00:00, 6.96MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/5], Loss: 0.1171\n",
            "Epoch [2/5], Loss: 0.1318\n",
            "Epoch [3/5], Loss: 0.0741\n",
            "Epoch [4/5], Loss: 0.0508\n",
            "Epoch [5/5], Loss: 0.0933\n",
            "Test Accuracy: 95.92%\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "\n",
        "\n",
        "# Load and preprocess the MNIST dataset\n",
        "transform = transforms.Compose([               # creates a pipeline of image transformations that will be applied to each image in the dataset. The two transformations:\n",
        "    transforms.ToTensor(),                     # Converts a PIL image or NumPy array to a PyTorch tensor. Also scales pixel values from [0, 255] to [0.0, 1.0]\n",
        "    transforms.Normalize((0.5,), (0.5,))])     # Normalizes the tensor using:(x - 0.5) / 0.5 → scales [0, 1] to [-1, 1]. (0.5,), (0.5,): The first tuple is the mean, and the second is the standard deviation.\n",
        "                                               # for Color images (RGB): Use per-channel normalization → (meanR, meanG, meanB), (stdR, stdG, stdB)\n",
        "\n",
        "train_dataset = datasets.MNIST(root='./data', train=True, transform=transform, download=True)  # root='./data' → save the data to the ./data directory. train=True → load the training data. download=True → download it if not already present\n",
        "test_dataset = datasets.MNIST(root='./data', train=False, transform=transform)              # train=False → means load the 10,000 test samples. the train parameter is a boolean flag that tells the loader: True → load training set. False → load test set\n",
        "\n",
        "train_loader = DataLoader(dataset=train_dataset, batch_size=32, shuffle=True)             # DataLoader: Efficient batch loading, Supports multi-threaded loading (using multiple CPU threads in parallel), Shuffling, sharding, and more. During training, the model updates its weights once per batch.\n",
        "test_loader = DataLoader(dataset=test_dataset, batch_size=32, shuffle=False)              # shuffle=True (only for training); Randomly shuffles the dataset at the beginning of each epoch. For testing, you typically don't shuffle. you want to evaluate it in a fixed, repeatable order.\n",
        "\n",
        "\n",
        "\n",
        "# Define the model\n",
        "class SimpleNN(nn.Module):                 # creates a new class SimpleNN that inherits from PyTorch’s nn.Module, the base class for all neural network models in PyTorch.\n",
        "    def __init__(self):                    # __init__ is the constructor method of the class. It initializes the layers of your network. This method gets called automatically when you create a new object from a class (like model = SimpleNN()).\n",
        "        super(SimpleNN, self).__init__()   # super() gives us access to the parent class (\"superclass\"). in this case, nn.Module, which SimpleNN inherits from.\n",
        "                                           # super().__init__(), Tells Python to call the __init__() method of the parent class nn.Module, so it can set up all the internal machinery needed for a PyTorch neural network.”\n",
        "                                           # self is a reference to the current object (or instance) of a class. When you define a class and then create an object from it, self lets that object refer to itself so it can access its own variables and methods.\n",
        "        self.flatten = nn.Flatten()        # This layer converts the 2D input images (28x28 pixels from MNIST) into 1D vectors of size 784 (28×28 = 784). It’s necessary because fully connected layers expect flat vectors.\n",
        "        self.fc1 = nn.Linear(28*28, 128)   #input size(28*28), output (128, number of neurons). First fully connected layer (\"dense layer\"):\n",
        "        self.fc2 = nn.Linear(128, 10)      # second fully connected layer (fc2) is the output layer. These 10 values are called logits, which represent unnormalized predictions\n",
        "\n",
        "\n",
        "    def forward(self, x):              # the forward() method defines the actual path your input data takes through the layers of the network. This function takes in x, the input tensor. self is the object itself, referring to the current instance of the model.\n",
        "        x = self.flatten(x)\n",
        "        x = torch.relu(self.fc1(x))    # This passes the input through the first fully connected (linear) layer (fc1). Then it applies ReLU activation, which adds non-linearity. ReLU means: \"Keep positive values, set all negative values to 0.\"\n",
        "        x = self.fc2(x)                # No activation here because the next step usually applies softmax or cross-entropy loss, which handles it internally.\n",
        "        return x                       # Returns the output logits (raw scores) from the network.\n",
        "\n",
        "model = SimpleNN()        # creates an instance of your neural network class, SimpleNN.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Define loss and optimizer\n",
        "criterion = nn.CrossEntropyLoss()    # loss function. doesn't need normalization. has sort of softmax built-in. It combines: Softmax, which converts raw scores (logits) from the output layer into probabilities. Negative Log Likelihood (NLL), which penalizes wrong predictions by comparing predicted probabilities to the correct class label.\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)    # optimizer that will update your model's weights during training, to minimize the loss.\n",
        "                                                        # model.parameters(): gives the optimizer access to all the learnable weights and biases in the model so it knows what to update.\n",
        "                                                        # lr=0.001: learning rate, which controls how big the updates to the weights are. A small learning rate like 0.001 means small, careful updates — good for stable learning.\n",
        "                                                        # If your learning rate is too low, training is slow and may get stuck in local minima. If it's too high, the model may not converge (never settle) or even diverge (get worse over time).\n",
        "\n",
        "# Check for GPU\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')           # train on GPU if available, otherwise   on CPU.\n",
        "model.to(device)        # Moves your model’s parameters and computations to the selected device.\n",
        "\n",
        "# Train the model\n",
        "num_epochs = 5\n",
        "for epoch in range(num_epochs):        # epoch loop\n",
        "    model.train()    # Tells PyTorch you’re training, not evaluating. Enables features like dropout and batch normalization (if your model uses them).\n",
        "    for images, labels in train_loader:          # batch loop\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(images)                # Passes the images through the model. outputs = model’s predictions (logits for each class).\n",
        "        loss = criterion(outputs, labels)      # calculating loss\n",
        "\n",
        "        # Backward and optimize\n",
        "        optimizer.zero_grad()                  # Clears previous gradients from the last step (important!). Otherwise, PyTorch would accumulate gradients across steps.\n",
        "        loss.backward()                        # Backpropagation: Computes gradients of the loss w.r.t. model’s weights.\n",
        "        optimizer.step()                       # Updates the model’s weights using gradients and learning rate\n",
        "\n",
        "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
        "\n",
        "# Evaluate the model\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for images, labels in test_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "    accuracy = 100 * correct / total\n",
        "    print(f'Test Accuracy: {accuracy:.2f}%')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "\n",
        "\n",
        "# Load and preprocess the MNIST dataset\n",
        "transform = transforms.Compose([               # creates a pipeline of image transformations that will be applied to each image in the dataset. The two transformations:\n",
        "    transforms.ToTensor(),                     # Converts a PIL image or NumPy array to a PyTorch tensor. Also scales pixel values from [0, 255] to [0.0, 1.0]\n",
        "    transforms.Normalize((0.5,), (0.5,))])     # Normalizes the tensor using:(x - 0.5) / 0.5 → scales [0, 1] to [-1, 1]. (0.5,), (0.5,): The first tuple is the mean, and the second is the standard deviation.\n",
        "                                               # for Color images (RGB): Use per-channel normalization → (meanR, meanG, meanB), (stdR, stdG, stdB)\n",
        "\n",
        "train_dataset = datasets.MNIST(root='./data', train=True, transform=transform, download=True)  # root='./data' → save the data to the ./data directory. train=True → load the training data. download=True → download it if not already present\n",
        "test_dataset = datasets.MNIST(root='./data', train=False, transform=transform)              # train=False → means load the 10,000 test samples. the train parameter is a boolean flag that tells the loader: True → load training set. False → load test set\n",
        "\n",
        "train_loader = DataLoader(dataset=train_dataset, batch_size=32, shuffle=True)             # DataLoader: Efficient batch loading, Supports multi-threaded loading (using multiple CPU threads in parallel), Shuffling, sharding, and more. During training, the model updates its weights once per batch.\n",
        "test_loader = DataLoader(dataset=test_dataset, batch_size=32, shuffle=False)              # shuffle=True (only for training); Randomly shuffles the dataset at the beginning of each epoch. For testing, you typically don't shuffle. you want to evaluate it in a fixed, repeatable order.\n",
        "\n",
        "\n",
        "\n",
        "# Define the model\n",
        "class SimpleNN(nn.Module):                 # creates a new class SimpleNN that inherits from PyTorch’s nn.Module, the base class for all neural network models in PyTorch.\n",
        "    def __init__(self):                    # __init__ is the constructor method of the class. It initializes the layers of your network. This method gets called automatically when you create a new object from a class (like model = SimpleNN()).\n",
        "        super(SimpleNN, self).__init__()   # super() gives us access to the parent class (\"superclass\"). in this case, nn.Module, which SimpleNN inherits from.\n",
        "                                           # super().__init__(), Tells Python to call the __init__() method of the parent class nn.Module, so it can set up all the internal machinery needed for a PyTorch neural network.”\n",
        "                                           # self is a reference to the current object (or instance) of a class. When you define a class and then create an object from it, self lets that object refer to itself so it can access its own variables and methods.\n",
        "        self.flatten = nn.Flatten()        # This layer converts the 2D input images (28x28 pixels from MNIST) into 1D vectors of size 784 (28×28 = 784). It’s necessary because fully connected layers expect flat vectors.\n",
        "        self.fc1 = nn.Linear(28*28, 128)   #input size(28*28), output (128, number of neurons). First fully connected layer (\"dense layer\"):\n",
        "        self.fc2 = nn.Linear(128, 10)      # second fully connected layer (fc2) is the output layer. These 10 values are called logits, which represent unnormalized predictions\n",
        "\n",
        "\n",
        "    def forward(self, x):              # the forward() method defines the actual path your input data takes through the layers of the network. This function takes in x, the input tensor. self is the object itself, referring to the current instance of the model.\n",
        "        x = self.flatten(x)\n",
        "        x = torch.relu(self.fc1(x))    # This passes the input through the first fully connected (linear) layer (fc1). Then it applies ReLU activation, which adds non-linearity. ReLU means: \"Keep positive values, set all negative values to 0.\"\n",
        "        x = self.fc2(x)                # No activation here because the next step usually applies softmax or cross-entropy loss, which handles it internally.\n",
        "        return x                       # Returns the output logits (raw scores) from the network.\n",
        "\n",
        "model = SimpleNN()        # creates an instance of your neural network class, SimpleNN.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Define loss and optimizer\n",
        "criterion = nn.CrossEntropyLoss()    # loss function. doesn't need normalization. has sort of softmax built-in. It combines: Softmax, which converts raw scores (logits) from the output layer into probabilities. Negative Log Likelihood (NLL), which penalizes wrong predictions by comparing predicted probabilities to the correct class label.\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)    # optimizer that will update your model's weights during training, to minimize the loss.\n",
        "                                                        # model.parameters(): gives the optimizer access to all the learnable weights and biases in the model so it knows what to update.\n",
        "                                                        # lr=0.001: learning rate, which controls how big the updates to the weights are. A small learning rate like 0.001 means small, careful updates — good for stable learning.\n",
        "                                                        # If your learning rate is too low, training is slow and may get stuck in local minima. If it's too high, the model may not converge (never settle) or even diverge (get worse over time).\n",
        "\n",
        "# Check for GPU\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')           # train on GPU if available, otherwise   on CPU.\n",
        "model.to(device)        # Moves your model’s parameters and computations to the selected device.\n",
        "\n",
        "# Train the model\n",
        "num_epochs = 5\n",
        "for epoch in range(num_epochs):        # epoch loop\n",
        "    model.train()    # Tells PyTorch you’re training, not evaluating. Enables features like dropout and batch normalization (if your model uses them).\n",
        "    running_loss = 0.0\n",
        "    for images, labels in train_loader:          # batch loop\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(images)                # Passes the images through the model. outputs = model’s predictions (logits for each class).\n",
        "        loss = criterion(outputs, labels)      # calculating loss\n",
        "\n",
        "        # Backward and optimize\n",
        "        optimizer.zero_grad()                  # Clears previous gradients from the last step (important!). Otherwise, PyTorch would accumulate gradients across steps.\n",
        "        loss.backward()                        # Backpropagation: Computes gradients of the loss w.r.t. model’s weights.\n",
        "        optimizer.step()                       # Updates the model’s weights using gradients and learning rate\n",
        "        running_loss += loss.item()            # .item(): prints a float\n",
        "\n",
        "    avg_loss = running_loss / len(train_loader)               # calculating the average loss for each epoch\n",
        "    print(f'Epoch [{epoch+1}/{num_epochs}], Average Loss: {avg_loss:.4f}')\n",
        "\n",
        "# Evaluate the model\n",
        "model.eval()           # Puts the model in evaluation mode\n",
        "with torch.no_grad():   # Temporarily turns off gradient tracking to save memory and speed up computations, since we’re not training here\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for images, labels in test_loader:        # Goes through the test dataset batch by batch.\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        outputs = model(images)          # Gets the model’s raw predictions (logits) for each image.\n",
        "        _, predicted = torch.max(outputs.data, 1)   # Selects the index of the highest score for each image. torch.max(tensor, dim) == torch.max(outputs.data, 1) finds the maximum value along a specific dimension (We're taking the maximum across columns (i.e., across classes) for each row/image.). Returns two things: 1-The maximum value (not needed here, so we assign it to _) 2- The index of that maximum → this is the predicted class.\n",
        "        total += labels.size(0)                     # counting how many total predictions were made. size(0): how many labels are in the batch.\n",
        "        correct += (predicted == labels).sum().item()   # predicted is tensor of class indices, one for each image in the batch. labels is a tensor containing the true class labels for a batch of images\n",
        "\n",
        "    accuracy = 100 * correct / total\n",
        "    print(f'Test Accuracy: {accuracy:.2f}%')"
      ],
      "metadata": {
        "id": "scuTj06dBKN8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9335a61b-ce3a-41bd-929a-61efc7a4471d"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9.91M/9.91M [00:11<00:00, 899kB/s] \n",
            "100%|██████████| 28.9k/28.9k [00:00<00:00, 134kB/s]\n",
            "100%|██████████| 1.65M/1.65M [00:01<00:00, 1.08MB/s]\n",
            "100%|██████████| 4.54k/4.54k [00:00<00:00, 6.09MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/5], Average Loss: 0.3516\n",
            "Epoch [2/5], Average Loss: 0.1786\n",
            "Epoch [3/5], Average Loss: 0.1320\n",
            "Epoch [4/5], Average Loss: 0.1093\n",
            "Epoch [5/5], Average Loss: 0.0939\n",
            "Test Accuracy: 96.80%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "q_dqSHFl4PMO"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}