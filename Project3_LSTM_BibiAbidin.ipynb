{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOnA1RJ+MRIxr5z6f4Mz0HR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fatemehabedin2/AIG/blob/main/Project3_LSTM_BibiAbidin.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Project 3: Implementing a Simple Recurrent Neural Network (RNN)\n",
        "\n",
        "## Introduction\n",
        "\n",
        "In this project, you will design, implement, and evaluate a simple Recurrent Neural Network (RNN) from scratch. This will involve building the entire pipeline, from data preprocessing to model training and evaluation.\n",
        "\n",
        "## Objectives\n",
        "\n",
        "1. Set up TensorFlow or PyTorch environments. You are free to choose your preferred DL platform.\n",
        "2. Use GPU for training.\n",
        "3. Create a data loader and implement data preprocessing where needed.\n",
        "4. Design a Convolutional Neural Network.\n",
        "5. Train and evaluate your model. Make sure to clearly show loss and accuracy values. Include visualizations too.\n",
        "6. Answer assessment questions."
      ],
      "metadata": {
        "id": "p6BwA3X2E910"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I am using text8 dataset fro kaggle to create a text generator."
      ],
      "metadata": {
        "id": "GNwdOEcyNtyA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Myg-OmaEnxy",
        "outputId": "4fd69fd7-10ea-4582-a55e-af4c6f9853b8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset downloaded to: /kaggle/input/text8-word-embedding\n",
            "Files in dataset:\n",
            "['text8']\n"
          ]
        }
      ],
      "source": [
        "import kagglehub\n",
        "import os\n",
        "\n",
        "path = kagglehub.dataset_download(\"gupta24789/text8-word-embedding\")\n",
        "\n",
        "# Check what files are downloaded\n",
        "print(\"Dataset downloaded to:\", path)\n",
        "print(\"Files in dataset:\")\n",
        "print(os.listdir(path))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "print(\"PyTorch version:\", torch.__version__)\n",
        "if torch.cuda.is_available():\n",
        "    print(\"GPU is available for PyTorch!\")\n",
        "else:\n",
        "    print(\"No GPU found for PyTorch.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zN8Suua2LDDv",
        "outputId": "190f07bf-0756-4724-db61-d816e8bed5da"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PyTorch version: 2.6.0+cu124\n",
            "GPU is available for PyTorch!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let’s:\n",
        "\n",
        "Read the file\n",
        "\n",
        "Display a sample of words\n",
        "\n",
        "Confirm the vocabulary size"
      ],
      "metadata": {
        "id": "a0R7Ema5OVaG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open(os.path.join(path, \"text8\"), \"r\") as file:\n",
        "    text = file.read()\n",
        "\n",
        "print(\"Length of raw text:\", len(text))\n",
        "print(\"First 500 characters:\", text[:500])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uqj8304mE7N1",
        "outputId": "2f4e2137-6972-423b-f138-2fd261ab9a94"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of raw text: 713069767\n",
            "First 500 characters:  anarchism originated as a term of abuse first used against early working class radicals including the diggers of the english revolution and the sans culottes of the french revolution whilst the term is still used in a pejorative way to describe any act that used violent means to destroy the organization of society it has also been taken up as a positive label by self defined anarchists the word anarchism is derived from the greek without archons ruler chief king anarchism as a political philoso\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "words = text.split()\n",
        "print(\"Total number of words:\", len(words))"
      ],
      "metadata": {
        "id": "dISda_8eOu53"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "unique_words = sorted(set(words))\n",
        "print(\"Vocabulary size:\", len(unique_words))"
      ],
      "metadata": {
        "id": "A9CY_l4fOuy7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Even though text8 is already lowercased, alphabetic-only, space-separated, I'll still use the standard practice for pre-processing step."
      ],
      "metadata": {
        "id": "MrY0u4PyYIJL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def preprocess_text(text):\n",
        "    text = text.lower()\n",
        "\n",
        "    # Removes anything that’s not a-z or whitespace. replace anything other than a-z or whitespace with \"\". 6 inside [] means anything other than\n",
        "    text = re.sub(r\"[^a-z\\s]\", \"\", text)\n",
        "\n",
        "    # Remove extra whitespace\n",
        "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
        "\n",
        "    return text\n",
        "\n",
        "\n",
        "clean_text = preprocess_text(text)\n",
        "len(clean_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "skbRCVnbYaFN",
        "outputId": "05186a5c-e261-4f07-ac34-64eb1928cacd"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "713069766"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "words = clean_text.split()\n",
        "\n",
        "print(\"First 20 words:\", words[:20])\n",
        "print(\"Total number of words:\", len(words))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ODsxtrguYaAt",
        "outputId": "96e0669e-f95b-41f8-ce4f-407e44b65359"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First 20 words: ['anarchism', 'originated', 'as', 'a', 'term', 'of', 'abuse', 'first', 'used', 'against', 'early', 'working', 'class', 'radicals', 'including', 'the', 'diggers', 'of', 'the', 'english']\n",
            "Total number of words: 124301826\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Word-Level Modeling\n",
        "I am going to use Word-Level Modeling, because it Learns semantic meaning better, so it would be more natural for real language modeling compared to Character-Level Modeling.\n",
        "\n",
        "# Vocabulary and Encoding\n",
        "Build the word2idx and idx2word dictionaries.\n",
        "\n",
        "word2idx: Word ➝ Integer is used during: Preprocessing / training and Model input creation.\n",
        "\n",
        "idx2word: Integer ➝ Word is used during: Text generation (predictions)\n",
        "and Evaluation / debugging\n",
        "\n"
      ],
      "metadata": {
        "id": "r23upfhrVaNH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "\n",
        "word_counts = Counter(words)\n",
        "\n",
        "# Creating sorted vocabulary for indexing\n",
        "vocab = sorted(word_counts.keys())\n",
        "\n",
        "word2idx = {word: idx for idx, word in enumerate(vocab)}\n",
        "idx2word = {idx: word for word, idx in word2idx.items()}\n",
        "\n",
        "# Encode whole text : full dataset as list of token IDs\n",
        "\n",
        "encoded_text = [word2idx[word] for word in words]\n",
        "\n",
        "print(\"First 20 encoded tokens:\", encoded_text[:20])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NPfL6nrdOurY",
        "outputId": "203f5052-2756-47d7-c449-eb1e536497b8"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First 20 encoded tokens: [26983, 534054, 43559, 0, 728186, 524750, 3359, 248070, 771709, 11156, 208483, 811779, 139584, 601589, 339492, 731197, 189025, 524750, 731197, 221580]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(encoded_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TxEUEIog6uIX",
        "outputId": "4a5a8711-77b3-493f-ce36-ff897ba1b071"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "124301826"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Use only 1M tokens\n",
        "encoded_text = encoded_text[:25000000]"
      ],
      "metadata": {
        "id": "f0DYXVS7YZ8l"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(encoded_text)"
      ],
      "metadata": {
        "id": "dpzoD2eM8PtE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_sequences(data, seq_length):\n",
        "    inputs = []\n",
        "    targets = []\n",
        "    for i in range(len(encoded_text) - seq_length):\n",
        "        inputs.append(encoded_text[i:i + seq_length])\n",
        "        targets.append(encoded_text[i + seq_length])\n",
        "    return inputs, targets\n",
        "\n",
        "seq_length = 20  # 20 words to predict the 21st\n",
        "inputs, targets = create_sequences(encoded_text, seq_length)\n",
        "\n",
        "# Convert to tensors\n",
        "inputs = torch.tensor(inputs, dtype=torch.long)   # torch.tensor() Takes any iterable (list, NumPy array, etc.) and copies the data into a new PyTorch tensor # Sets the data type explicitly to torch.long = 64-bit integer (A 1D tensor of int64 integers)\n",
        "targets = torch.tensor(targets, dtype=torch.long)\n",
        "\n",
        "print(\"Input shape:\", inputs.shape)\n",
        "print(\"Target shape:\", targets.shape)\n",
        "print(\"Example input:\", inputs[0])\n",
        "print(\"Target word index:\", targets[0])\n",
        "print(\"Target word:\", idx2word[targets[0].item()])"
      ],
      "metadata": {
        "id": "DMVF6NWnhy4c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "After running above code my colab session crashed with the message: \"Your session crashed after using all available RAM\", because we're generating millions of sequences, and storing each as a list inside inputs.\n",
        "\n",
        "Now instead of preloading all sequences, I try to create them on-the-fly in the Dataset class. This way we don't store all sequences in memory."
      ],
      "metadata": {
        "id": "gCCxTPckyB_N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "di2Hq8lix9Cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Streaming Dataset + DataLoader"
      ],
      "metadata": {
        "id": "syHs9N7xEpKe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class Text8StreamingDataset(Dataset):\n",
        "    def __init__(self, encoded_text, sequence_length):\n",
        "        self.data = encoded_text\n",
        "        self.seq_len = sequence_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data) - self.seq_len\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        x = torch.tensor(self.data[idx:idx + self.seq_len], dtype=torch.long)\n",
        "        y = torch.tensor(self.data[idx + self.seq_len], dtype=torch.long)\n",
        "        return x, y\n",
        "\n",
        "\n",
        "sequence_length = 20\n",
        "batch_size = 32     # DataLoader will Select 32 random indices. For each of those 32 indices, It calls dataset’s __getitem__(idx): Now we have 32 sequences of 20 words (x_batch: [32, 20], y_batch:[32])\n",
        "\n",
        "# Create train and validation DataLoaders\n",
        "train_dataset = Text8StreamingDataset(train_data, sequence_length)\n",
        "val_dataset   = Text8StreamingDataset(val_data, sequence_length)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
        "val_loader   = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=0)     # Colab often crashes when num_workers > 0 in DataLoader.\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OvFoYRSFn9Lm",
        "outputId": "a7b38e44-814d-4d34-ee7d-0fa668c3a866"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input batch shape: torch.Size([32, 20])\n",
            "Target batch shape: torch.Size([32])\n",
            "First input sequence (word indices): tensor([ 19646, 733490, 801564, 437113, 253600, 771447, 461099, 529311,  27554,\n",
            "        253600, 538048, 640409, 307325, 110242, 200118, 259710, 307909, 105481,\n",
            "        731197, 580487])\n",
            "First target word: of\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LSTM Language Model\n",
        "\n",
        "We'll build a basic LSTM that:\n",
        "\n",
        "Takes a sequence of word indices [32, 20]\n",
        "\n",
        "Embeds them to vectors (like word2vec)\n",
        "\n",
        "Feeds them through LSTM layers\n",
        "\n",
        "Outputs a logit for each word in the vocabulary\n"
      ],
      "metadata": {
        "id": "YoK3sWRfFVU5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class LSTMLanguageModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_layers=2):      # input_size=embedding_dim, hidden_size=hidden_dim, output_size=vocab_size\n",
        "        super(LSTMLanguageModel, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)       # Each word index maps to a dense vectors (embeddings). Every word ID gets a trainable vector of fixed size, like weights, trained along with our model, change every epoch, optimized via backpropagation,and learn to capture semantic similarity.\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers, batch_first=True)    # batch_first=True >> input shape = [batch_size, seq_len, embedding_dim] >> more intuitive and Compatible with DataLoader batches\n",
        "        self.fc = nn.Linear(hidden_dim, vocab_size)         # hidden_dim: The size of the hidden state vector for each LSTM unit at every time step\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: [batch_size, seq_len]\n",
        "        embedded = self.embedding(x)       # [batch_size, seq_len, embedding_dim]\n",
        "        lstm_out, _ = self.lstm(embedded)  # lstm_out: [batch_size, seq_len, hidden_dim]\n",
        "        final_hidden = lstm_out[:, -1, :]  # Take output from last timestep → [batch_size, hidden_dim]\n",
        "        logits = self.fc(final_hidden)     # → [batch_size, vocab_size]\n",
        "        return logits\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 109
        },
        "id": "virJ6wbE3vgC",
        "outputId": "108c42ad-1ed6-42eb-d9ac-ec4e3c33c88a"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "unterminated string literal (detected at line 2) (ipython-input-9-1404640251.py, line 2)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipython-input-9-1404640251.py\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    We'll build a basic LSTM that:\u001b[0m\n\u001b[0m      ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unterminated string literal (detected at line 2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The output size is vocab_size because our model is doing classification over the entire vocabulary. it predicts the next word by selecting from all possible words."
      ],
      "metadata": {
        "id": "yvy2VCq-TIV_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train the LSTM Language Model\n",
        "we’ll create the full training loop that:\n",
        "\n",
        "Loads batches of sequences\n",
        "\n",
        "Feeds them to the model\n",
        "\n",
        "Computes loss using CrossEntropyLoss\n",
        "\n",
        "Backpropagates\n",
        "\n",
        "Updates weights using Adam"
      ],
      "metadata": {
        "id": "jb-uR9axWtzA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_dim = 128\n",
        "hidden_dim = 256\n",
        "vocab_size = len(word2idx)\n",
        "\n",
        "model = LSTMLanguageModel(vocab_size, embedding_dim, hidden_dim)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n"
      ],
      "metadata": {
        "id": "khNWuB2ZFUdc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Use GPU if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)\n",
        "\n",
        "num_epochs = 3\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    total_loss = 0\n",
        "    model.train()  # Set model to training mode\n",
        "\n",
        "    for batch_idx, (x_batch, y_batch) in enumerate(dataloader):\n",
        "        x_batch = x_batch.to(device)\n",
        "        y_batch = y_batch.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        logits = model(x_batch)  # Output: [batch_size, vocab_size]\n",
        "\n",
        "        # Compute loss\n",
        "        loss = criterion(logits, y_batch)\n",
        "\n",
        "        # Backpropagation\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        # Print progress every N batches\n",
        "        if batch_idx % 100 == 0:\n",
        "            print(f\"Epoch [{epoch+1}/{num_epochs}], Batch [{batch_idx}], Loss: {loss.item():.4f}\")\n",
        "\n",
        "    avg_loss = total_loss / len(dataloader)\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}] complete. Avg loss: {avg_loss:.4f}\")\n"
      ],
      "metadata": {
        "id": "HQuRBOH-XJ_D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "num_epochs = 5\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "\n",
        "model.to(device)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    train_loss = 0\n",
        "\n",
        "    for x_batch, y_batch in train_loader:\n",
        "        x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
        "\n",
        "        logits = model(x_batch)\n",
        "        loss = criterion(logits, y_batch)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "\n",
        "    avg_train_loss = train_loss / len(train_loader)\n",
        "    train_losses.append(avg_train_loss)\n",
        "\n",
        "    # Validation loop\n",
        "    model.eval()\n",
        "    val_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for x_val, y_val in val_loader:\n",
        "            x_val, y_val = x_val.to(device), y_val.to(device)\n",
        "            logits = model(x_val)\n",
        "            loss = criterion(logits, y_val)\n",
        "            val_loss += loss.item()\n",
        "\n",
        "    avg_val_loss = val_loss / len(val_loader)\n",
        "    val_losses.append(avg_val_loss)\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs} | Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f}\")\n"
      ],
      "metadata": {
        "id": "oG4jWKSfgsvI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.plot(train_losses, label='Train Loss')\n",
        "plt.plot(val_losses, label='Validation Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training vs Validation Loss')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "iGeW-8JkhIcB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}